{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Spiral_Experiments.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "fXWKM7vOccDv"
      },
      "source": [
        "\"\"\"\n",
        "Here is the code used to carry out the experiments on the two coiling spirals: error_rate(epsilon). We average the value over 1000 runs. \n",
        "Reader can find also the code of our network architecture.\n",
        "We used a dataset (x_train,y_train,x_test,y_test) generated by the notebook 'Generate_Spiral_dataset'. \n",
        "One can generate others datasets (noise level, number of samples) with the notebook 'Generate_Spiral_dataset'.\n",
        "\"\"\"\n",
        "\n",
        "import random\n",
        "import os\n",
        "import scipy.stats as st\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
        "\n",
        "import torch\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from matplotlib.colors import ListedColormap\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nBtvQW_5dAEp"
      },
      "source": [
        "### Dataset \n",
        "class bis_spiral(Dataset):\n",
        "  def __init__(self, x_train, x_test, y_train, y_test , train = True):\n",
        "        if train:\n",
        "          self.X = x_train\n",
        "          self.Y = y_train\n",
        "        else:\n",
        "          self.X = x_test\n",
        "          self.Y = y_test\n",
        "  def __len__(self):\n",
        "          'Denotes the total number of samples'\n",
        "          return len(self.X)\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "          x = torch.tensor(self.X[index]).float()\n",
        "          y = torch.tensor(self.Y[index]).float()\n",
        "          return x,y,index\n",
        "\n",
        "def create_data(x_train, x_test, y_train, y_test,batch_size ):\n",
        "                                                                                                                                                                        \n",
        "  transformed_dataset = bis_spiral(x_train, x_test, y_train, y_test, train = True)\n",
        "  train_loader =  DataLoader(transformed_dataset, batch_size, shuffle=True)\n",
        "  test_loader = DataLoader(bis_spiral(x_train, x_test, y_train, y_test ,train = False),batch_size =  int(n_samples*0.2),shuffle=True)\n",
        "  return train_loader, test_loader\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oZUXI2XZfjcW"
      },
      "source": [
        "### Load and plot the spiral dataset\n",
        "path = './data/spirals_dataset'\n",
        "\n",
        "cm_bright = ListedColormap(['#FF0000', '#0000FF'])\n",
        "x_train, x_test, y_train, y_test = np.load(\"x_train.npy\"), np.load(\"x_test.npy\"),np.load(\"y_train.npy\"),np.load(\"y_test.npy\")\n",
        "fig = plt.figure()\n",
        "plt.scatter(x_train[:,0],x_train[:,1],c=y_train[:,0],cmap=cm_bright,edgecolors='k')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H9PPqyDXczLX"
      },
      "source": [
        "###Network\n",
        "class Net(nn.Module):\n",
        "    # define nn                                                                                                                                                                                        \n",
        "    def __init__(self,Ncouche = 50):\n",
        "        super(Net, self).__init__()\n",
        "        self.fc1 = nn.Linear(2, Ncouche )\n",
        "        self.fc2 = nn.Linear(Ncouche, Ncouche )\n",
        "        self.fc3= nn.Linear(Ncouche, Ncouche )\n",
        "        self.fc4 = nn.Linear(Ncouche , 2)\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "\n",
        "    def forward(self, X, y = None,mixup = False,eps  =1, alpha =1):\n",
        "        if mixup:\n",
        "          X, y_a, y_b, lam,dist = mixup_data (X,y,eps = eps)\n",
        "        else:\n",
        "          y_a, y_b, lam = None,None,None\n",
        "\n",
        "          dist = torch.ones(X.size()[0])\n",
        "\n",
        "        X = F.relu(self.fc1(X))\n",
        "        X = F.relu(self.fc2(X))\n",
        "        X = F.relu(self.fc3(X))\n",
        "        X = self.fc4(X)\n",
        "        X = self.softmax(X)\n",
        "\n",
        "        return X,y_a,y_b,lam,dist\n",
        "\n",
        "\n",
        "def mixup_data(x, y, eps = 0, alpha=1.0, use_cuda=False,):\n",
        "\n",
        "    '''Compute the mixup data. Return mixed inputs, pairs of targets, and lambda'''\n",
        "    if alpha > 0.:\n",
        "        lam = np.random.beta(alpha, alpha)\n",
        "    else:\n",
        "        lam = 1.\n",
        "    batch_size = x.size()[0]\n",
        "    if use_cuda:\n",
        "        index = torch.randperm(batch_size).cuda()\n",
        "    else:\n",
        "        index = torch.randperm(batch_size)\n",
        "\n",
        "    mixed_x = lam * x + (1 - lam) * x[index,:]\n",
        "\n",
        "    #we compute the weights here. If d> eps => w = 0.\n",
        "    m = nn.Threshold(eps,0,inplace=False)\n",
        "    Cdist = torch.norm(x.flatten(start_dim = 1)-x[index,:].flatten(start_dim = 1),p=2,dim =1) #Euclidean distance between x and x permuted\n",
        "    dist = torch.heaviside(-m(Cdist),torch.tensor(1.)) #we apply the threshold. \n",
        "    y_a, y_b = y, y[index]\n",
        "    return mixed_x, y_a, y_b, lam,dist\n",
        " \n",
        "def mix_criterion(criterion, pred, y_a, y_b,lam):\n",
        "\n",
        "    return lam * criterion(pred, y_a) + (1 - lam) * criterion(pred, y_b)                                                                                                                                  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CHo0WIb4gy03"
      },
      "source": [
        "criterion = nn.CrossEntropyLoss()# cross entropy loss         \n",
        "def experiment(net,train_loader,cost_index= None,mixup = False, eps = 0,lr = 0.01,epochs = 10 ,alpha =1,verbose=False):\n",
        "  optimizer = torch.optim.Adam(net.parameters(), lr=0.01)\n",
        "  scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=300, gamma=0.1)\n",
        "\n",
        "  Loss=[]\n",
        "  for epoch in range(epochs):\n",
        "    for i,(x,y,index) in enumerate(train_loader):\n",
        "      optimizer.zero_grad()\n",
        "      y_onehot = torch.FloatTensor(y.shape[0], 2)\n",
        "      y_onehot.zero_()\n",
        "      if eps == 0:\n",
        "        mixup = False\n",
        "      if mixup:\n",
        "        out, y_a,y_b,lam,dist = net(x, y, mixup = mixup, eps = eps)\n",
        "        loss = mix_criterion(criterion, out,y_a.flatten().long(),y_b.flatten().long(),lam)\n",
        "      else:\n",
        "        out, y_a, y_b , lam, dist = net(x,y)\n",
        "        loss = criterion(out, y.flatten().long())\n",
        "      loss = (loss * dist).mean() #The loss is weighted by dist as described in our paper.\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "    scheduler.step()\n",
        "\n",
        "  mean_accuracy_scores = []\n",
        "  for (x_test,y_test,__) in test_loader:\n",
        "    predict_out,__,__,__ ,__= net(x_test)\n",
        "\n",
        "\n",
        "    _, predict_y = torch.max(predict_out, 1)\n",
        "\n",
        "    if verbose:\n",
        "      print ('prediction accuracy', accuracy_score(y_test, predict_y.data))\n",
        "\n",
        "      print( 'macro precision', precision_score(y_test, predict_y.data, average='macro'))\n",
        "      print ('micro precision', precision_score(y_test, predict_y.data, average='micro'))\n",
        "      print ('macro recall', recall_score(y_test, predict_y.data, average='macro'))\n",
        "      print ('micro recall', recall_score(y_test, predict_y.data, average='micro'))\n",
        "      #plot_decision_boundary(x_grid,y_grid,x_test,y_test,net)                                                                                                                                         \n",
        "    mean_accuracy_scores.append(accuracy_score(y_test, predict_y.data))\n",
        "  return np.mean(mean_accuracy_scores)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "et2-fXqzioFz"
      },
      "source": [
        "n_runs = 1000\n",
        "\n",
        "Epsilon = [0,0.6,1,1.5,2,2.5,3,3.5,4,10]\n",
        "Batch = [20,241,103,50,33,25,22,20,20,20] ###Determined by Experiments \n",
        "\n",
        "mean_accuracy =[]\n",
        "\n",
        "for nb,eps in enumerate(Epsilon):\n",
        "  mean_accuracy_eps = []\n",
        "  for i in range(n_runs):\n",
        "    train_loader, test_loader = create_data(x_train, x_test, y_train, y_test,Batch[nb])\n",
        "    network_mixup_graph = Net(100)\n",
        "    test_accuracy = experiment(network_mixup_graph,train_loader,epochs= 200, eps = eps, mixup = True,verbose=False,)                                                                                       \n",
        "    mean_accuracy_eps.append(test_accuracy)                                                                                                                                                                            \n",
        "  mean_accuracy.append(np.mean(mean_accuracy_eps))\n",
        "  print(mean_accuracy)\n",
        "  print(st.norm.interval(0.95, loc = np.mean(mean_accuracy), scale = st.sem(A)))\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}