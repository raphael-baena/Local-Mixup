{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Programme clean resnet18 + cifar 10\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using pytorch version: 1.9.0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import scipy.stats as st\n",
    "from torchvision import datasets, transforms\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "import math\n",
    "import time\n",
    "import argparse\n",
    "\n",
    "print(\"Using pytorch version: \" + torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "def is_interactive():\n",
    "    import __main__ as main\n",
    "    return not hasattr(main, '__file__')\n",
    "    \n",
    "parser = argparse.ArgumentParser(description='Cifar quick code')\n",
    "parser.add_argument(\"--batch-size\", default=64, help=\"batch_size\")\n",
    "parser.add_argument(\"--device\", default=\"cuda:0\", help=\"device to use\")\n",
    "parser.add_argument(\"--feature-maps\", default=64, help=\"number of feature maps\")\n",
    "parser.add_argument(\"--quiet\", action=\"store_true\", help=\"prevent too much display of info\")\n",
    "parser.add_argument(\"--dataset-path\", default=os.environ.get(\"DATASETS\"), help=\"dataset path\")\n",
    "if is_interactive:\n",
    "    args = parser.parse_args(args = [])\n",
    "else:\n",
    "    args = parser.parse_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset():\n",
    "    def __init__(self, data, targets, shuffle = True, transforms = [], batch_size = args.batch_size, device = args.device):\n",
    "        self.data = data.to(device)\n",
    "        self.targets = targets.to(device)\n",
    "        assert(data.shape[0] == targets.shape[0])\n",
    "        self.shuffle = shuffle\n",
    "        self.length = data.shape[0]\n",
    "        self.batch_size = batch_size\n",
    "        self.transforms = transforms\n",
    "    def __iter__(self):\n",
    "        if self.shuffle:\n",
    "            perm = np.random.permutation(np.arange(self.length))\n",
    "            self.data = self.data[perm]\n",
    "            self.targets = self.targets[perm]\n",
    "        for i in range(self.length // self.batch_size):\n",
    "            data, targets = self.data[i * self.batch_size : (i+1) * self.batch_size], self.targets[i * self.batch_size : (i+1) * self.batch_size]\n",
    "            data = self.transforms(data)\n",
    "            yield data, targets\n",
    "        if self.length % self.batch_size != 0:\n",
    "            data, targets = self.data[self.length - (self.length % self.batch_size):], self.targets[self.length - (self.length % self.batch_size):]\n",
    "            data = self.transforms(data)\n",
    "            yield data, targets\n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "    def batch_length(self):\n",
    "        return self.length // self.batch_size + (0 if self.length % self.batch_size == 0 else 1)\n",
    "    \n",
    "def mnist(batch_size):\n",
    "\n",
    "    transform = transforms.Compose([\n",
    "           transforms.ToTensor(),\n",
    "           transforms.Normalize((0.1307,), (0.3081,))\n",
    "        ])\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        datasets.MNIST(path, train=True, download=True, transform=transform),\n",
    "        batch_size = batch_size, shuffle=True\n",
    "    )\n",
    "\n",
    "    test_loader = torch.utils.data.DataLoader(\n",
    "        datasets.MNIST(path, train=False, download=True, transform=transform),\n",
    "        batch_size = batch_size, shuffle=False\n",
    "    )\n",
    "    \n",
    "    return (train_loader, train_loader, test_loader), [1, 28, 28], 10\n",
    "\n",
    "def cifar10(data_augmentation = True):\n",
    "    train_loader = datasets.CIFAR10(args.dataset_path, train = True, download = True)\n",
    "    train_data = torch.stack(list(map(transforms.ToTensor(), train_loader.data)))\n",
    "    train_targets = torch.LongTensor(train_loader.targets)\n",
    "    test_loader = datasets.CIFAR10(args.dataset_path, train = False, download = True)\n",
    "    test_data = torch.stack(list(map(transforms.ToTensor(), test_loader.data)))\n",
    "    test_targets = torch.LongTensor(test_loader.targets)\n",
    "    if data_augmentation:\n",
    "        list_trans_train = [        \n",
    "            transforms.RandomCrop(32, padding=4),\n",
    "            transforms.RandomHorizontalFlip()\n",
    "        ]\n",
    "    else:\n",
    "        list_trans_train = []\n",
    "    list_trans = [\n",
    "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
    "    ]\n",
    "    train_loader = Dataset(train_data, train_targets, transforms = transforms.Compose(list_trans_train + list_trans))\n",
    "    val_loader = Dataset(train_data, train_targets, transforms = transforms.Compose(list_trans))\n",
    "    test_loader = Dataset(test_data, test_targets, transforms = transforms.Compose(list_trans))\n",
    "    return (train_loader, val_loader, test_loader), [3, 32, 32], 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Définition du modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "    def __init__(self, in_planes, planes, stride=1):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != self.expansion*planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(self.expansion*planes)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += self.shortcut(x)\n",
    "        return F.relu(out)\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, num_blocks, feature_maps, num_classes=10):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.in_planes = feature_maps\n",
    "        self.length = len(num_blocks)\n",
    "        self.conv1 = nn.Conv2d(3, feature_maps, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(feature_maps)\n",
    "        layers = []\n",
    "        for i, nb in enumerate(num_blocks):\n",
    "            layers.append(self._make_layer(block, (2 ** i) * feature_maps, nb, stride = 1 if i == 0 else 2))            \n",
    "        self.layers = nn.Sequential(*layers)\n",
    "        self.linear = nn.Linear((2 ** (len(num_blocks) - 1)) * feature_maps * block.expansion, num_classes)        \n",
    "        self.depth = len(num_blocks)\n",
    "        \n",
    "    def _make_layer(self, block, planes, num_blocks, stride):\n",
    "        strides = [stride] + [1]*(num_blocks-1)\n",
    "        layers = []\n",
    "        for i in range(len(strides)):\n",
    "            stride = strides[i]\n",
    "            layers.append(block(self.in_planes, planes, stride))\n",
    "            if i < len(strides) - 1:\n",
    "                layers.append(nn.ReLU())\n",
    "            self.in_planes = planes * block.expansion\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        for i in range(len(self.layers)):\n",
    "            out = self.layers[i](out)\n",
    "        out = F.avg_pool2d(out, out.shape[2])\n",
    "        features = out.view(out.size(0), -1)\n",
    "        out = self.linear(features)\n",
    "        return out, features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Routines d'entraînement et de test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "last_update = 0\n",
    "\n",
    "def train(model, train_loader, optimizer, mixup = \"None\"):\n",
    "    model.train()\n",
    "    global last_update\n",
    "    accuracy, total_loss, total_elts = 0., 0., 0\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        #if mixup == \"standard\":\n",
    "        #    index_mixup = torch.randperm(data.shape[0])\n",
    "        #if mixup == \"graph\":\n",
    "        #    ins = data.reshape(data.shape[0], -1)\n",
    "        #    dists = torch.norm(ins.reshape(data.shape[0], 1, -1) - ins.reshape(1, data.shape[0], -1), p = 2, dim = 2)\n",
    "        #    indices = torch.sort(dists, dim = 1)[1]\n",
    "        #    index_mixup = torch.zeros(dists.shape[0], dtype = torch.long).to(device)\n",
    "        #    for i in range(data.shape[0]):\n",
    "        #        index_mixup[i] = indices[i, random.randint(0, min(k, data.shape[0] - 1))]\n",
    "        if mixup == \"standard\":\n",
    "            index_mixup = torch.randperm(data.shape[0])\n",
    "        if mixup == \"local\":\n",
    "            data_v = data.reshape(data.shape[0], -1)\n",
    "            distances = torch.norm(data.reshape(data.shape[0], 1, -1) - data.reshape(1, data.shape[0], -1), p = 2, dim = 2)\n",
    "            sim = torch.exp(-1 * alpha * distances)\n",
    "            sim = sim / torch.sum(sim, dim = 1, keepdim = True)\n",
    "            index_mixup = torch.LongTensor(data.shape[0])\n",
    "            for index in range(data.shape[0]):\n",
    "                probas = sim[index].numpy()\n",
    "                probas = probas / np.sum(probas)\n",
    "                index_mixup[index] = np.random.choice(np.arange(data.shape[0]), p=probas)\n",
    "        if mixup == \"standard\" or mixup == \"local\":\n",
    "            lam = random.random() / 2\n",
    "            data_mixed = lam * data + (1 - lam) * data[index_mixup]\n",
    "            output, _ = model(data_mixed)\n",
    "            loss = lam * criterion(output, target) + (1 - lam) * criterion(output, target[index_mixup])            \n",
    "        else:\n",
    "            output, _ = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            \n",
    "        loss.backward()\n",
    "        total_loss += loss.item() * data.shape[0]\n",
    "        total_elts += data.shape[0]\n",
    "        optimizer.step()\n",
    "        if time.time() - last_update > 0.1 and not args.quiet:\n",
    "            print(\"\\r{:5d}/{:5d} loss:{:.5f} time:{:9d}.{:02d}\".format(batch_idx + 1, train_loader.batch_length(), total_loss / total_elts, int(time.time() - start_time), int(100*(time.time() - start_time)) % 100), end = \"\")\n",
    "            last_update = time.time()\n",
    "        \n",
    "    return { \"train_loss\" : total_loss / len(train_loader) }\n",
    "\n",
    "def test(model, test_loader):\n",
    "    model.eval()\n",
    "    test_loss, accuracy = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            output, _ = model(data)\n",
    "            test_loss += criterion(output, target).item() * data.shape[0]\n",
    "            pred = output.argmax(dim=1, keepdim=True)\n",
    "            accuracy += pred.eq(target.view_as(pred)).sum().item()\n",
    "    return { \"test_loss\" : test_loss / len(test_loader), \"test_acc\" : accuracy / len(test_loader) }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_features(loader, filename, pre = False):\n",
    "    all_features = []\n",
    "    all_targets = []\n",
    "    elements_per_class = torch.zeros(10, dtype=torch.long)\n",
    "    for batch_idx, (data, target) in enumerate(loader):\n",
    "        with torch.no_grad():\n",
    "            output, features = model(data)\n",
    "            dim = features.shape[1]\n",
    "            all_features.append(features.cpu())\n",
    "            all_targets.append(target.cpu())\n",
    "            for i in range(target.shape[0]):\n",
    "                elements_per_class[target[i]] += 1\n",
    "\n",
    "    max_elements_per_class = torch.max(elements_per_class).item()\n",
    "    features = torch.zeros((10, max_elements_per_class, dim))\n",
    "    elements_per_class[:] = 0\n",
    "    for i in range(len(all_features)):\n",
    "        for j in range(all_features[i].shape[0]):\n",
    "            features[all_targets[i][j], elements_per_class[all_targets[i][j]]] = all_features[i][j]\n",
    "            elements_per_class[all_targets[i][j]] += 1\n",
    "    torch.save(features, filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Routine de test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_era(model, epochs, lr, loaders, mixup = False, verbose = False):\n",
    "    if lr < 0:\n",
    "        optimizer = torch.optim.Adam(model.parameters())\n",
    "    else:\n",
    "        optimizer = torch.optim.SGD(model.parameters(), lr = lr, momentum = 0.9, weight_decay = 5e-4)        \n",
    "    train_loader, val_loader, test_loader = loaders\n",
    "    for epoch in range(epochs):\n",
    "        train_stats = train(model, train_loader, optimizer, mixup = mixup)\n",
    "        test_stats = test(model, test_loader)\n",
    "        print(\"\\rEpoch: {:3d}, test_acc: {:.2f}%, train_loss: {:.5f}\".format(epoch, 100 * test_stats[\"test_acc\"], train_stats[\"train_loss\"]), end = \"\")\n",
    "        if verbose:\n",
    "            print()\n",
    "    print()\n",
    "    return test_stats[\"test_acc\"]\n",
    "\n",
    "def train_complete(model, training, loaders, mixup = False):\n",
    "    global start_time\n",
    "    start_time = time.time()\n",
    "    for (epochs, lr) in training:\n",
    "        test_acc = train_era(model, epochs, lr, loaders, mixup = mixup)\n",
    "    return test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "    1/  782 loss:2.34462 time:        1.05\n",
      "    4/  782 loss:4.64492 time:        1.30\n",
      "    7/  782 loss:5.81336 time:        1.55\n",
      "   10/  782 loss:5.37597 time:        1.79\n",
      "   13/  782 loss:5.03286 time:        2.05\n",
      "   16/  782 loss:4.75405 time:        2.30\n",
      "   19/  782 loss:4.41376 time:        2.55\n",
      "   22/  782 loss:4.19395 time:        2.80\n",
      "   25/  782 loss:3.99099 time:        3.06\n",
      "   28/  782 loss:3.81918 time:        3.32\n",
      "   31/  782 loss:3.68021 time:        3.57\n",
      "   34/  782 loss:3.56787 time:        3.82\n",
      "   37/  782 loss:3.47329 time:        4.06\n",
      "   40/  782 loss:3.38728 time:        4.34\n",
      "   43/  782 loss:3.31535 time:        4.60\n",
      "   46/  782 loss:3.25021 time:        4.87\n",
      "   49/  782 loss:3.19172 time:        5.13\n",
      "   52/  782 loss:3.13998 time:        5.39\n",
      "   55/  782 loss:3.09449 time:        5.64\n",
      "   58/  782 loss:3.05255 time:        5.89\n",
      "   61/  782 loss:3.01369 time:        6.13\n",
      "   64/  782 loss:2.97900 time:        6.38\n",
      "   67/  782 loss:2.94587 time:        6.63\n",
      "   70/  782 loss:2.91558 time:        6.88\n",
      "   73/  782 loss:2.88827 time:        7.15\n",
      "   76/  782 loss:2.86253 time:        7.40\n",
      "   79/  782 loss:2.83973 time:        7.65\n",
      "   82/  782 loss:2.81683 time:        7.90\n",
      "   85/  782 loss:2.79542 time:        8.15\n",
      "   88/  782 loss:2.77267 time:        8.40\n",
      "   91/  782 loss:2.75596 time:        8.64\n",
      "   94/  782 loss:2.73810 time:        8.89\n",
      "   97/  782 loss:2.72190 time:        9.14\n",
      "  100/  782 loss:2.70648 time:        9.39\n",
      "  103/  782 loss:2.69315 time:        9.64\n",
      "  106/  782 loss:2.67960 time:        9.89\n",
      "  109/  782 loss:2.66552 time:       10.13\n",
      "  112/  782 loss:2.65095 time:       10.39\n",
      "  115/  782 loss:2.63707 time:       10.63\n",
      "  118/  782 loss:2.62679 time:       10.88\n",
      "  121/  782 loss:2.61659 time:       11.15\n",
      "  124/  782 loss:2.60708 time:       11.42\n",
      "  127/  782 loss:2.59640 time:       11.66\n",
      "  130/  782 loss:2.58663 time:       11.91\n",
      "  133/  782 loss:2.57656 time:       12.18\n",
      "  136/  782 loss:2.56676 time:       12.43\n",
      "  139/  782 loss:2.55646 time:       12.68\n",
      "  142/  782 loss:2.54706 time:       12.93\n",
      "  145/  782 loss:2.54000 time:       13.17\n",
      "  148/  782 loss:2.53438 time:       13.42\n",
      "  151/  782 loss:2.52725 time:       13.67\n",
      "  154/  782 loss:2.51902 time:       13.92\n",
      "  157/  782 loss:2.51329 time:       14.16\n",
      "  160/  782 loss:2.50728 time:       14.42\n",
      "  163/  782 loss:2.50153 time:       14.67\n",
      "  166/  782 loss:2.49634 time:       14.92\n",
      "  169/  782 loss:2.49053 time:       15.17\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_52246/816882596.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mResNet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBasicBlock\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_maps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mscores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_complete\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmixup\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"standard\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minterval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.95\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_52246/184993836.py\u001b[0m in \u001b[0;36mtrain_complete\u001b[0;34m(model, training, loaders, mixup)\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0mtest_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_era\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmixup\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmixup\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtest_acc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_52246/184993836.py\u001b[0m in \u001b[0;36mtrain_era\u001b[0;34m(model, epochs, lr, loaders, mixup, verbose)\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloaders\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0mtrain_stats\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmixup\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmixup\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m         \u001b[0mtest_stats\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\rEpoch: {:3d}, test_acc: {:.2f}%, train_loss: {:.5f}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mtest_stats\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"test_acc\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_stats\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"train_loss\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_52246/1001797112.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, train_loader, optimizer, mixup)\u001b[0m\n\u001b[1;32m     33\u001b[0m             \u001b[0mlam\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m             \u001b[0mdata_mixed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlam\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlam\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex_mixup\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m             \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_mixed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlam\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlam\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex_mixup\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_52246/1471126970.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mavg_pool2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.9/site-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.9/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1845\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhas_torch_function_variadic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1846\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mhandle_torch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1847\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1848\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1849\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "training = [(100, 0.1), (100, 0.01), (100, 0.001), (50, 0.0001)]\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "loaders, _, _ = cifar10(data_augmentation = True)\n",
    "\n",
    "alpha = 1e-3\n",
    "\n",
    "scores = []\n",
    "\n",
    "for i in range(100):\n",
    "    model = ResNet(BasicBlock, [2, 2, 2, 2], args.feature_maps).to(args.device)\n",
    "    scores.append(train_complete(model, training, loaders, mixup = \"standard\"))\n",
    "    print(np.mean(scores), st.norm.interval(0.95, loc = np.mean(scores), scale = st.sem(scores)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
