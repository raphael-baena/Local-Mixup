# -*- coding: utf-8 -*-
"""Cifar10_Lipschitz.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1djFUkOIq6394qiAlatO4wWw6EbwcIbQT
"""

"""
Here is the code used to estimate the Lipschitz lower bound on Cifar for Vanilla, Local Mixup and Mixup.
For Local Mixup user chooses the value of Epsilon (*EPS*) 
For Mixup user must choose a very large value of EPS ,e.g EPS = 1000.
For Vanilla user runs directly "min_dist".
"""


import torch
import numpy as np
from torchvision import datasets, transforms
import os
import torch.nn.functional as F
import time

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(device)

batch_size = 64

path = './data'

list_trans = [
    transforms.ToTensor(),
    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))
]

transform_test = transforms.Compose(list_trans)

loader = torch.utils.data.DataLoader(
        datasets.CIFAR10(path, train=True, download=True, transform=transform_test),
        batch_size = batch_size, shuffle=True, num_workers = 2)
def get_all_samples():
    result = []
    targets = []
    targets_long = []
    for i, (data, target) in enumerate(loader):
        result.append(data.reshape(data.shape[0], -1).to(device))
        targets.append(F.one_hot(target, num_classes = 10).float().to(device))
        targets_long.append(target.float().to(device))
    return torch.cat(result, dim = 0), torch.cat(targets, dim = 0), torch.cat(targets_long, dim = 0)
samples, targets, targets_long = get_all_samples()

def min_dist(batch_size):
    assert(samples.shape[0] % batch_size == 0)
    runs = samples.shape[0] // batch_size
    min_distance = 10000000
    start_tick = time.time()
    last_tick = 0
    for i in range(runs):
        dists = torch.norm(samples[batch_size * i : batch_size * (i+1)].reshape(batch_size, 1, -1) - samples.reshape(1, samples.shape[0], -1), p = 2, dim = 2).reshape(-1)
        classes = torch.einsum("bs,Bs->bB", targets[batch_size * i : batch_size * (i+1)], targets).reshape(-1)
        new_dist = torch.min(dists[torch.where(classes == 0)[0]]).item()
        if new_dist < min_distance:
            min_distance = new_dist
        timelapse = time.time() - start_tick
        if int(timelapse) > last_tick:
            print("\r", i, "/", runs, min_distance, int(timelapse), int((runs - i) * timelapse / (i+1)), end = "                     ")
            last_tick = int(timelapse)

def min_dist_mixup(batch_size):
    assert((samples.shape[0] // 10) % batch_size == 0)
    runs = (samples[:].shape[0] // 10) // batch_size
    data = []
    for i in range(10):
        data.append(samples[torch.where(targets_long == i)[0]][:].reshape(1, -1, samples.shape[1]))
    data = torch.cat(data, dim = 0)
    print(data.size())
    min_distance = 10000000
    start_tick = time.time()  
    for c1 in range(10): # class of initial point                                                                                                                                                       
        print("starting with initial point in class ", c1)
        for c2 in range(10):
            print("segment in class ", c2)
            if c1 != c2:# class of target segment (here we consider only cases where the two interpolated points are of the same class)                                                                 
                for i in range(runs):
                    samples_a = data[c1, batch_size * i : batch_size * (i+1)]
                    for j in range(runs):
                        samples_b = data[c2, batch_size * j : batch_size * (j+1)]
                        segments = data[c2].reshape(data[c2].shape[0], 1, -1) - samples_b.reshape(1, samples_b.shape[0], -1)
                        norm_segm = torch.norm(segments, dim = 2, keepdim = True)
                        segments_normalized = segments / norm_segm #torch.norm(segments, dim = 2, keepdim = True)                                                                                       
                        offset = norm_segm-EPS
                        penality_segm = torch.heaviside(offset,torch.tensor(0.))*100000
                        segments_between_b_and_a = samples_a.reshape(samples_a.shape[0], 1, -1) - samples_b.reshape(1, samples_b.shape[0], -1)
                        scalars = torch.einsum("abd,cbd->acb", segments_between_b_and_a, segments_normalized)
                        scalars = torch.clamp(scalars, min = 0)
                        length_segments = torch.norm(segments, dim = 2).reshape(1, segments.shape[0], segments.shape[1]) - scalars
                        length_segments = torch.clamp(length_segments, min = 0)
                        length_segments = (length_segments > 0).float()
                        scalars = scalars * length_segments
                        dists = torch.pow(torch.pow(torch.norm(segments_between_b_and_a, dim = 2).reshape(segments_between_b_and_a.shape[0], 1, -1),2) - torch.pow(scalars,2), 0.5)
                        dists = torch.nan_to_num(dists, nan = 100000000.) + penality_segm.flatten(start_dim = 1).repeat(samples_a.size()[0],1,1)
                        new_min = torch.min(dists).item()
                        min_distance = min(new_min, min_distance)
                    timelapse = time.time() - start_tick
                    print("\r", c1, c2, i, "/", runs, min_distance, int(timelapse), int((runs - i) * timelapse / (i+1)), end ='                     ')

EPS  = 35 ### Value of EPS
min_dist_mixup(10)